behaviors:
  FoxAgent:                                 # MUST match Behavior Name in Unity
    trainer_type: ppo                       # {ppo, sac, poca} — algorithm

    # -------------------- Hyperparameters --------------------
    hyperparameters:
      batch_size: 1024                      # SGD minibatch size per epoch (B)  (was 4096)
      buffer_size: 32768                    # Rollout size per update (≫ batch_size)  (was 131072)
      learning_rate: 1.0e-3                 # α in Adam optimizer (faster early learning)
      beta: 3.0e-3                          # β in L_total = ... - β·H[π]  (entropy bonus)
      epsilon: 0.2                           # ε in L_clip = E[min(rA, clip(r,1±ε)A)]  (PPO clip)
      lambd: 0.95                            # λ in GAE: A_t = Σ_k (γλ)^k δ_{t+k}
      num_epoch: 3                           # K — passes over buffer each update (was 4)
      learning_rate_schedule: linear         # {constant, linear} — LR schedule

    # -------------------- Network --------------------
    network_settings:
      normalize: true                        # Normalize vector obs online
      hidden_units: 256                      # MLP width for vector/ray features (was 512)
      num_layers: 4                          # MLP depth (was 4)
      vis_encode_type: simple                # {simple, nature_cnn, resnet, match3} — cheaper encoder for grayscale RT
      # memory:                              # (optional) recurrent policy/value
      #   sequence_length: 64                # BPTT unroll length
      #   memory_size: 128                   # LSTM hidden size

    # -------------------- Reward signals --------------------
    reward_signals:
      extrinsic:
        gamma: 0.995                         # γ in G_t = Σ_k γ^k r_{t+k+1} and GAE
        strength: 1.0                        # Scale multiplier on this reward
      #curiosity:                           # (optional) intrinsic motivation — enable only if exploration is a problem
      #  strength: 0.02                     # scales intrinsic r_int
      #  gamma: 0.99                        # discount for intrinsic return

    # -------------------- Optional: Behavioral Cloning (imitation) --------------------
    behavioral_cloning:
      demo_path: Demos/                      # .demo recorded in Unity (set your path)
      strength: 0.6                          # Add L_BC with this weight to total loss (0.3–0.7 typical)
      steps: 500000                          # Apply BC for first N steps, then PPO runs alone
      # samples_per_update: 0                # (default: all) — usually omit

    # -------------------- Training cadence / logging --------------------
    max_steps: 5.0e6                         # Stop after this many env steps (across all envs)
    time_horizon: 128                        # Truncation T for returns/GAE (affects bias/var)
    summary_freq: 5000                       # Steps between TB logs (lower = more frequent)
    keep_checkpoints: 3                      # # of .onnx checkpoints to keep

# -------------------- Engine / device --------------------
torch_settings:
  device: cuda                               # {cpu, cuda} — use GPU if available

# -------------------- Optional: Curriculum --------------------
# environment_parameters:
#   difficulty:
#     curriculum:
#       - name: easy
#         completion_criteria:
#           measure: reward                   # Promote when smoothed reward ≥ threshold
#           behavior: FoxAgent
#           signal_smoothing: true
#           min_lesson_length: 200
#           threshold: 0.7
#         value: 0                            # Read via Academy in C#
#       - name: medium
#         completion_criteria:
#           measure: reward
#           behavior: FoxAgent
#           signal_smoothing: true
#           min_lesson_length: 200
#           threshold: 1.2
#         value: 1
#       - name: hard
#         value: 2

# -------------------- Reference (math) --------------------
# Importance ratio: r_t(θ) = π_θ(a_t|s_t) / π_{θ_old}(a_t|s_t)
# Clipped PPO objective: L_clip(θ) = E_t[ min( r_t A_t, clip(r_t, 1-ε, 1+ε) A_t ) ]
# Policy loss:          L_policy = - L_policy_clip
# Policy entropy:       H[π] = - Σ_a π(a|s) log π(a|s)
# Value loss:           L_value = (V_θ - V_target)^2
# Advantage (GAE):      A_t = Σ_k (γλ)^k δ_{t+k},     δ_t = r_{t+1} + γ V(s_{t+1}) - V(s_t)
# Target value:         V_target_t = A_t + V(s_t)
